{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:14.303552474Z",
     "start_time": "2024-01-12T08:49:12.258475917Z"
    }
   },
   "source": [
    "%matplotlib notebook\n",
    "import haiku as hk\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "import numpy as np\n",
    "import tree\n",
    "\n",
    "from PIL import Image\n",
    "from tapnet import tapir_model\n",
    "from tapnet.utils import transforms\n",
    "from tapnet.utils import viz_utils"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Checkpoint"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55212468b69e6f4f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "checkpoint_path = 'tapnet/checkpoints/tapir_checkpoint_panning.npy'\n",
    "ckpt_state = np.load(checkpoint_path, allow_pickle=True).item()\n",
    "params, state = ckpt_state['params'], ckpt_state['state']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:18.364599350Z",
     "start_time": "2024-01-12T08:49:18.137700892Z"
    }
   },
   "id": "971290b55b0e518b",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d8fd34fba34bdc4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def build_model(frames, query_points):\n",
    "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
    "  model = tapir_model.TAPIR(bilinear_interp_with_depthwise_conv=False, pyramid_level=0)\n",
    "  outputs = model(\n",
    "      video=frames,\n",
    "      is_training=False,\n",
    "      query_points=query_points,\n",
    "      query_chunk_size=64,\n",
    "  )\n",
    "  return outputs\n",
    "\n",
    "model = hk.transform_with_state(build_model)\n",
    "model_apply = jax.jit(model.apply)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:21.354509143Z",
     "start_time": "2024-01-12T08:49:21.234626033Z"
    }
   },
   "id": "deb627a52b4c7b5e",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utility Functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "788bcc702189766d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def preprocess_frames(frames):\n",
    "  \"\"\"Preprocess frames to model inputs.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "\n",
    "  Returns:\n",
    "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
    "  \"\"\"\n",
    "  frames = frames.astype(np.float32)\n",
    "  frames = frames / 255 * 2 - 1\n",
    "  return frames\n",
    "\n",
    "\n",
    "def postprocess_occlusions(occlusions, expected_dist):\n",
    "  \"\"\"Postprocess occlusions to boolean visible flag.\n",
    "\n",
    "  Args:\n",
    "    occlusions: [num_points, num_frames], [-inf, inf], np.float32\n",
    "    expected_dist: [num_points, num_frames], [-inf, inf], np.float32\n",
    "\n",
    "  Returns:\n",
    "    visibles: [num_points, num_frames], bool\n",
    "  \"\"\"\n",
    "  visibles = (1 - jax.nn.sigmoid(occlusions)) * (1 - jax.nn.sigmoid(expected_dist)) > 0.5\n",
    "  return visibles\n",
    "\n",
    "def inference(frames, query_points):\n",
    "  \"\"\"Inference on one video.\n",
    "\n",
    "  Args:\n",
    "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
    "    query_points: [num_points, 3], [0, num_frames/height/width], [t, y, x]\n",
    "\n",
    "  Returns:\n",
    "    tracks: [num_points, 3], [-1, 1], [t, y, x]\n",
    "    visibles: [num_points, num_frames], bool\n",
    "  \"\"\"\n",
    "  # Preprocess video to match model inputs format\n",
    "  frames = preprocess_frames(frames)\n",
    "  num_frames, height, width = frames.shape[0:3]\n",
    "  query_points = query_points.astype(np.float32)\n",
    "  frames, query_points = frames[None], query_points[None]  # Add batch dimension\n",
    "\n",
    "  # Model inference\n",
    "  rng = jax.random.PRNGKey(42)\n",
    "  outputs, _ = model_apply(params, state, rng, frames, query_points)\n",
    "  outputs = tree.map_structure(lambda x: np.array(x[0]), outputs)\n",
    "  tracks, occlusions, expected_dist = outputs['tracks'], outputs['occlusion'], outputs['expected_dist']\n",
    "\n",
    "  # Binarize occlusions\n",
    "  visibles = postprocess_occlusions(occlusions, expected_dist)\n",
    "  return tracks, visibles\n",
    "\n",
    "\n",
    "def sample_random_points(frame_max_idx, height, width, num_points):\n",
    "  \"\"\"Sample random points with (time, height, width) order.\"\"\"\n",
    "  y = np.random.randint(0, height, (num_points, 1))\n",
    "  x = np.random.randint(0, width, (num_points, 1))\n",
    "  t = np.random.randint(0, frame_max_idx + 1, (num_points, 1))\n",
    "  points = np.concatenate((t, y, x), axis=-1).astype(np.int32)  # [num_points, 3]\n",
    "  return points"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:23.598441467Z",
     "start_time": "2024-01-12T08:49:23.480068353Z"
    }
   },
   "id": "459c2b4d6a6cfdec",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Videos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2600579fa0d6455"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "video = media.read_video('Preprocessed_Videos/AI_GeMo_late_F-/35_F-_2_c.mp4')\n",
    "height, width = video.shape[1:3]\n",
    "media.show_video(video, fps=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:33.816816266Z",
     "start_time": "2024-01-12T08:49:25.998267241Z"
    }
   },
   "id": "fb90317878722468",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(height, width) "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:36.213535252Z",
     "start_time": "2024-01-12T08:49:36.002139588Z"
    }
   },
   "id": "b59e9c1030a35024",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Select Any Points at Any Frame"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:39.326597073Z",
     "start_time": "2024-01-12T08:49:39.198087942Z"
    }
   },
   "id": "2d64c61d863dbfd4",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "select_frame = 0  #@param {type:\"slider\", min:0, max:49, step:1}\n",
    "\n",
    "# Generate a colormap with 20 points, no need to change unless select more than 20 points\n",
    "colormap = viz_utils.get_colors(20)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(video[select_frame])\n",
    "ax.axis('off')\n",
    "ax.set_title('You can select more than 1 points. After select enough points, run the next cell.')\n",
    "\n",
    "select_points = []\n",
    "\n",
    " # Event handler for mouse clicks\n",
    "def on_click(event):\n",
    "  if event.button == 1 and event.inaxes == ax:  # Left mouse button clicked\n",
    "    x, y = int(np.round(event.xdata)), int(np.round(event.ydata))\n",
    "\n",
    "    select_points.append(np.array([x, y]))\n",
    "\n",
    "    color = colormap[len(select_points) - 1]\n",
    "    color = tuple(np.array(color) / 255.0)\n",
    "    ax.plot(x, y, 'o', color=color, markersize=5)\n",
    "    plt.draw()\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:49:46.870031135Z",
     "start_time": "2024-01-12T08:49:46.805057935Z"
    }
   },
   "id": "17b3e6a747009fc5",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Predict Point Tracks for the Selected Points"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "afed7c58e7b44ab6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "print(select_points)\n",
    "# TODO: write selected points in global coordinates in file"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:50:09.695172556Z",
     "start_time": "2024-01-12T08:50:09.658794394Z"
    }
   },
   "id": "291ec6ed3cf3a831",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def expand2square(pil_img, background_color):\n",
    "    width, height = pil_img.size\n",
    "    if width == height:\n",
    "        return pil_img\n",
    "    elif width > height:\n",
    "        result = Image.new(pil_img.mode, (width, width), background_color)\n",
    "        result.paste(pil_img, (0, (width - height) // 2))\n",
    "        return result\n",
    "    else:\n",
    "        result = Image.new(pil_img.mode, (height, height), background_color)\n",
    "        result.paste(pil_img, ((height - width) // 2, 0))\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:50:19.312106046Z",
     "start_time": "2024-01-12T08:50:19.291654049Z"
    }
   },
   "id": "14e31243b75169ea",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#-- crop and resize image to 256x256 based on selected points\n",
    "resize_height = 256  # @param {type: \"integer\"}\n",
    "resize_width = 256  # @param {type: \"integer\"}\n",
    "\n",
    "# get video metadata\n",
    "fps_value = video.metadata.fps\n",
    "bps_value = video.metadata.bps\n",
    "\n",
    "# get extreme selected points\n",
    "x_coords = [pt[0] for pt in select_points]\n",
    "y_coords = [pt[1] for pt in select_points]\n",
    "y_max, y_min = max(y_coords), min(y_coords) # get top- and bottom-most selected points\n",
    "x_max, x_min = max(x_coords), min(x_coords) # get right- and left-most selected points\n",
    "\n",
    "# get spread of selected points\n",
    "x_spread = x_max-x_min\n",
    "y_spread = y_max-y_min\n",
    "\n",
    "#- crop around selected points\n",
    "margin = 0.15 # 15% margin\n",
    "x_margin = round(margin*x_spread)\n",
    "y_margin = round(margin*y_spread)\n",
    "largest_margin = max(x_margin, y_margin)\n",
    "\n",
    "min_x_crop, max_x_crop = max(0, x_min-largest_margin-1), min(width, x_max+largest_margin+1)\n",
    "min_y_crop, max_y_crop  = max(0, y_min-largest_margin-1), min(height, y_max+largest_margin+1)\n",
    "\n",
    "cropped_width = max_x_crop-min_x_crop\n",
    "cropped_height = max_y_crop-min_y_crop\n",
    "\n",
    "frames_original = video.__array__()\n",
    "frames_cropped = frames_original[:,min_y_crop:max_y_crop,min_x_crop:max_x_crop,:]\n",
    "\n",
    "#- expand to square (background padding) and resize to desired dimensions\n",
    "frames_crop_squared = []\n",
    "frames_crop_resized = []\n",
    "for frame in frames_cropped:\n",
    "    im = Image.fromarray(frame, mode=\"RGB\")\n",
    "    squared_frame = expand2square(im, (255, 255, 255))\n",
    "    resized_frame = squared_frame.resize((resize_height, resize_width))\n",
    "    frames_crop_squared.append(np.asarray(squared_frame))\n",
    "    frames_crop_resized.append(np.asarray(resized_frame))\n",
    "\n",
    "media.write_video('cropped_vid.mp4', frames_cropped, fps=fps_value, bps=bps_value)\n",
    "media.write_video('cropped_resized_vid.mp4', frames_crop_resized, fps=fps_value, bps=bps_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:50:25.971573080Z",
     "start_time": "2024-01-12T08:50:20.872282433Z"
    }
   },
   "id": "2dd99f05c7ede0ce",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def convert_select_points_to_query_points(frame, points):\n",
    "  \"\"\"Convert select points to query points.\n",
    "\n",
    "  Args:\n",
    "    points: [num_points, 2], in [x, y]\n",
    "  Returns:\n",
    "    query_points: [num_points, 3], in [t, y, x]\n",
    "  \"\"\"\n",
    "  points = np.stack(points)\n",
    "  query_points = np.zeros(shape=(points.shape[0], 3), dtype=np.float32)\n",
    "  query_points[:, 0] = frame\n",
    "  query_points[:, 1] = points[:, 1]\n",
    "  query_points[:, 2] = points[:, 0]\n",
    "  return query_points\n",
    "\n",
    "\n",
    "frames = media.resize_video(video, (resize_height, resize_width))\n",
    "query_points = convert_select_points_to_query_points(select_frame, select_points) #-DC: still in global reference: from xy to tyx\n",
    "\n",
    "\n",
    "\n",
    "# draw query_points on original image\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(np.array(video[select_frame]))\n",
    "color_list = [tuple(np.array(colormap[ii - 1])/ 255.0) for ii in range(len(select_points))]\n",
    "ax.scatter(query_points[:, 2], query_points[:, 1], marker=\"o\", color=color_list, s=25)\n",
    "plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:50:42.414976080Z",
     "start_time": "2024-01-12T08:50:40.498864434Z"
    }
   },
   "id": "94a72e78877f6206",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "# shift selected points according to cropped image\n",
    "query_points_crop = np.array([[0.0, cc[1]-min_y_crop, cc[2]-min_x_crop] for cc in query_points])\n",
    "\n",
    "# draw query_points on cropped image\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(np.array(frames_cropped[select_frame]))\n",
    "ax.scatter(query_points_crop[:, 2], query_points_crop[:, 1], marker=\"o\", color=color_list, s=25)\n",
    "plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:50:52.998590182Z",
     "start_time": "2024-01-12T08:50:52.957186085Z"
    }
   },
   "id": "a29d1cb7902b2f98",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# shift according to squared\n",
    "if cropped_width == cropped_height:\n",
    "    query_points_sq = query_points_crop\n",
    "elif cropped_width > cropped_height:\n",
    "    y_shift = (cropped_width - cropped_height) // 2\n",
    "    query_points_sq = np.array([[0.0, cc[1]+y_shift, cc[2]] for cc in query_points_crop])\n",
    "else:\n",
    "    x_shift = (cropped_height - cropped_width) // 2\n",
    "    query_points_sq = np.array([[0.0, cc[1], cc[2]+x_shift] for cc in query_points_crop])\n",
    "\n",
    "# draw query_points on squared image\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.imshow(np.array(frames_crop_squared[select_frame]))\n",
    "ax.scatter(query_points_sq[:, 2], query_points_sq[:, 1], marker=\"o\", color=color_list, s=25)\n",
    "plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:51:00.116248759Z",
     "start_time": "2024-01-12T08:51:00.076252961Z"
    }
   },
   "id": "28c04364fdeaf67e",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "query_points_fin = transforms.convert_grid_coordinates(query_points_sq, (1, max(cropped_height, cropped_width), max(cropped_height, cropped_width)), (1, resize_height, resize_width), coordinate_format='tyx')\n",
    "\n",
    "# draw query_points on cropped and resized image\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.imshow(np.array(frames_crop_resized[select_frame]))\n",
    "plt.scatter(query_points_fin[:, 2], query_points_fin[:, 1], marker=\"o\", color=color_list, s=25)\n",
    "plt.show(block=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:51:23.402601146Z",
     "start_time": "2024-01-12T08:51:23.370607653Z"
    }
   },
   "id": "423f8dd4845c2fb4",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Track points"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3323ddbcdeea0f1b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "tracks, visibles = inference(np.stack(frames_crop_resized, axis=0), query_points_fin)\n",
    "\n",
    "# Visualize sparse point tracks\n",
    "tracks = transforms.convert_grid_coordinates(tracks, (resize_width, resize_height), (max(cropped_height, cropped_width), max(cropped_height, cropped_width)))\n",
    "print(\"done 1\")\n",
    "video_viz = viz_utils.paint_point_track(np.stack(frames_crop_squared, axis=0), tracks, visibles, colormap)\n",
    "print(\"done 2\")\n",
    "media.write_video('tracked_points_video.mp4', video_viz, fps=fps_value, bps=bps_value)\n",
    "print(\"done 3\")\n",
    "# media.show_video(video_viz, fps=30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-12T08:52:46.717352526Z",
     "start_time": "2024-01-12T08:51:26.362016186Z"
    }
   },
   "id": "52374712e4709acc",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "72369ef6af07bef5",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
