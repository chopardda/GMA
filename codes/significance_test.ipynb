{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3f96bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f33197b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE_A = [\"early\", \"late\"]\n",
    "FEATURE_TYPES = [\"coordinates\", \"angles\", \"both\"]\n",
    "MODELS = [\"RandomForestClassifier\", \"LSTMModel\", \"CNN1D\"]\n",
    "METRIC = [\"Test/Test AUROC\", \"Test/Test AUPR\", \"Test/Test accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "503da75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_runs_iterations_dict(wandb_api: wandb.Api, project_str: str, name_filter: str = \"\") -> dict:\n",
    "    iterations_dict = {}\n",
    "    \n",
    "    for type_a in TYPE_A:\n",
    "        iterations_dict[type_a] = {}\n",
    "        for feature_type in FEATURE_TYPES:\n",
    "            iterations_dict[type_a][feature_type] = {}\n",
    "            for model in MODELS:\n",
    "                iterations_dict[type_a][feature_type][model] = { metric: [] for metric in METRIC }\n",
    "                runs = wandb_api.runs(\n",
    "                    path=project_str,\n",
    "                    filters={\n",
    "                        \"config.type_a\": type_a,\n",
    "                        \"config.feature_type\": feature_type,\n",
    "                        \"config.model\": model\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                for run in runs:\n",
    "                    if name_filter and name_filter not in run.name:\n",
    "                        continue\n",
    "                    run_summary = run.summary\n",
    "                    for metric in METRIC:\n",
    "                        if metric in run_summary:\n",
    "                            iterations_dict[type_a][feature_type][model][metric].append(run_summary[metric])\n",
    "                        else:\n",
    "                            print(f\"Metric {metric} not found in run {run.id}\")\n",
    "    \n",
    "    return iterations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69c7334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_p_test(group1, group2):\n",
    "    \"\"\"\n",
    "    Perform a two-sample t-test between two groups.\n",
    "    \n",
    "    Args:\n",
    "        group1 (list or np.array): First group of data.\n",
    "        group2 (list or np.array): Second group of data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: t-statistic and p-value.\n",
    "    \"\"\"\n",
    "    t_stat, p_val = ttest_ind(group1, group2)\n",
    "    return t_stat, p_val\n",
    "\n",
    "# Example data\n",
    "# group1 = np.random.normal(0.81055, 0.11545, 90)\n",
    "# group2 = np.random.normal(0.5076, 0.19, 30)\n",
    "\n",
    "# # Two-sample t-test\n",
    "# t_stat, p_val = ttest_ind(group1, group2)\n",
    "# print(f\"Two-sample t-test: t={t_stat}, p={p_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31efe7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkcc\u001b[0m (\u001b[33mkchincheong\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Get resuls for label and track\n",
    "os.environ['HTTP_PROXY'] = \"socks5h://localhost:10080\"\n",
    "os.environ['WANDB_API_KEY'] = \"<LOCAL_WANDB_API_KEY>\"\n",
    "wandb_host = \"http://wandb-vogtlab.leomed.ethz.ch:1337\"\n",
    "wandb.login(host=wandb_host)\n",
    "api = wandb.Api()\n",
    "label_and_track_iterations = get_runs_iterations_dict(api, \"kchincheong/GMA Results Tuned 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60fb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Aggpose results\n",
    "import os\n",
    "import wandb\n",
    "os.environ.unsetenv('HTTP_PROXY')\n",
    "os.environ['WANDB_API_KEY'] = \"<WANDB_API_KEY>\"\n",
    "wandb.login(host=\"https://api.wandb.ai\")\n",
    "api = wandb.Api()\n",
    "aggpose_iterations = get_runs_iterations_dict(api, \"dachopard/GMA Project\", \"best_params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863e3aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate table values\n",
    "table_value_dict = {}\n",
    "\n",
    "for type_a in TYPE_A:\n",
    "        table_value_dict[type_a] = {}\n",
    "        for feature_type in FEATURE_TYPES:\n",
    "            table_value_dict[type_a][feature_type] = {}\n",
    "            for model in MODELS:\n",
    "                table_value_dict[type_a][feature_type][model] = { metric: {} for metric in METRIC }\n",
    "                \n",
    "                for metric in METRIC:\n",
    "                    label_and_track_values = label_and_track_iterations[type_a][feature_type][model][metric]\n",
    "                    aggpose_values = aggpose_iterations[type_a][feature_type][model][metric]\n",
    "                    \n",
    "                    if len(label_and_track_values) > 0 and len(aggpose_values) > 0:\n",
    "                        t_stat, p_val = do_p_test(label_and_track_values, aggpose_values)\n",
    "                        table_value_dict[type_a][feature_type][model][metric]['p-value'] = p_val\n",
    "                        table_value_dict[type_a][feature_type][model][metric]['diff'] = np.mean(label_and_track_values) - np.mean(aggpose_values)\n",
    "                    else:\n",
    "                        print(f\"No data for {type_a}, {feature_type}, {model}, {metric}. Skipping p-test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee1f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences\n",
      "Test/Test AUROC\n",
      "& Coord. & $\\textbf{0.1005}$ & $\\textbf{0.1226}$ & $\\textbf{0.2578}$ & $0.0463$ & $-0.0889$ & $0.0071$ \\\\\n",
      "& Angles & $\\textbf{0.2641}$ & $0.0075$ & $\\textbf{0.2546}$ & $\\textbf{0.1511}$ & $\\textbf{-0.1146}$ & $\\textbf{-0.1625}$ \\\\\n",
      "& Both & $\\textbf{0.2122}$ & $\\textbf{0.1091}$ & $\\textbf{0.2383}$ & $0.0735$ & $\\textbf{-0.1737}$ & $0.0342$ \\\\\n",
      "\n",
      "Test/Test AUPR\n",
      "& Coord. & $\\textbf{0.0890}$ & $\\textbf{0.0954}$ & $\\textbf{0.1865}$ & $-0.0452$ & $\\textbf{-0.1412}$ & $0.0008$ \\\\\n",
      "& Angles & $\\textbf{0.1623}$ & $-0.0290$ & $\\textbf{0.1500}$ & $0.0878$ & $\\textbf{-0.1529}$ & $\\textbf{-0.2629}$ \\\\\n",
      "& Both & $\\textbf{0.1587}$ & $\\textbf{0.0857}$ & $\\textbf{0.1591}$ & $-0.0005$ & $\\textbf{-0.2397}$ & $-0.0207$ \\\\\n",
      "\n",
      "Test/Test accuracy\n",
      "& Coord. & $0.0671$ & $0.0540$ & $\\textbf{0.1980}$ & $\\textbf{0.0939}$ & $-0.0122$ & $-0.0277$ \\\\\n",
      "& Angles & $\\textbf{0.1912}$ & $-0.0235$ & $\\textbf{0.2000}$ & $\\textbf{0.0504}$ & $\\textbf{-0.0693}$ & $\\textbf{-0.0876}$ \\\\\n",
      "& Both & $\\textbf{0.1271}$ & $\\textbf{0.0987}$ & $\\textbf{0.1863}$ & $0.0426$ & $0.0143$ & $0.0098$ \\\\\n",
      "\n",
      "\n",
      "p-values\n",
      "Test/Test AUROC\n",
      "& Coord. & $\\textbf{0.0213}$ & $\\textbf{0.0016}$ & $\\textbf{0.0000}$ & $0.3859$ & $0.0908$ & $0.8785$ \\\\\n",
      "& Angles & $\\textbf{0.0000}$ & $0.8431$ & $\\textbf{0.0000}$ & $\\textbf{0.0027}$ & $\\textbf{0.0192}$ & $\\textbf{0.0030}$ \\\\\n",
      "& Both & $\\textbf{0.0000}$ & $\\textbf{0.0069}$ & $\\textbf{0.0000}$ & $0.1381$ & $\\textbf{0.0004}$ & $0.5183$ \\\\\n",
      "\n",
      "Test/Test AUPR\n",
      "& Coord. & $\\textbf{0.0070}$ & $\\textbf{0.0036}$ & $\\textbf{0.0000}$ & $0.2423$ & $\\textbf{0.0063}$ & $0.9875$ \\\\\n",
      "& Angles & $\\textbf{0.0000}$ & $0.3985$ & $\\textbf{0.0000}$ & $0.0909$ & $\\textbf{0.0037}$ & $\\textbf{0.0000}$ \\\\\n",
      "& Both & $\\textbf{0.0000}$ & $\\textbf{0.0108}$ & $\\textbf{0.0000}$ & $0.9901$ & $\\textbf{0.0000}$ & $0.7011$ \\\\\n",
      "\n",
      "Test/Test accuracy\n",
      "& Coord. & $0.0547$ & $0.0840$ & $\\textbf{0.0000}$ & $\\textbf{0.0031}$ & $0.6917$ & $0.4870$ \\\\\n",
      "& Angles & $\\textbf{0.0000}$ & $0.4485$ & $\\textbf{0.0000}$ & $\\textbf{0.0427}$ & $\\textbf{0.0248}$ & $\\textbf{0.0043}$ \\\\\n",
      "& Both & $\\textbf{0.0005}$ & $\\textbf{0.0072}$ & $\\textbf{0.0000}$ & $0.1324$ & $0.6787$ & $0.7724$ \\\\\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print difference table rows (latex)\n",
    "feature_type_map = {\n",
    "    \"coordinates\": \"Coord.\",\n",
    "    \"angles\": \"Angles\",\n",
    "    \"both\": \"Both\"\n",
    "}\n",
    "\n",
    "print(\"Differences\")\n",
    "for metric in METRIC:\n",
    "    print(metric)\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        line = f\"& {feature_type_map[feature_type]}\"\n",
    "        for type_a in TYPE_A:\n",
    "            for model in MODELS:\n",
    "                val = table_value_dict[type_a][feature_type][model][metric]['diff']\n",
    "                val_str = f\"\\\\textbf{{{val:.4f}}}\" if table_value_dict[type_a][feature_type][model][metric]['p-value'] < 0.05 else f\"{val:.4f}\"\n",
    "                line += f\" & ${val_str}$\"\n",
    "\n",
    "        line += \" \\\\\\\\\"\n",
    "        \n",
    "        print(line)\n",
    "    print(\"\")\n",
    "\n",
    "print(\"\")\n",
    "# Print p-value table rows (latex)\n",
    "print(\"p-values\")\n",
    "for metric in METRIC:\n",
    "    print(metric)\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        line = f\"& {feature_type_map[feature_type]}\"\n",
    "        for type_a in TYPE_A:\n",
    "            for model in MODELS:\n",
    "                val = table_value_dict[type_a][feature_type][model][metric]['p-value']\n",
    "                val_str = f\"\\\\textbf{{{val:.4f}}}\" if table_value_dict[type_a][feature_type][model][metric]['p-value'] < 0.05 else f\"{val:.4f}\"\n",
    "                line += f\" & ${val_str}$\"\n",
    "\n",
    "        line += \" \\\\\\\\\"\n",
    "        \n",
    "        print(line)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d48b2c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences\n",
      "Test/Test AUROC\n",
      "| | Coord. |  **0.1005** **0.1226** **0.2578** | 0.0463 -0.0889 0.0071 |\n",
      "| | Angles |  **0.2641** 0.0075 **0.2546** | **0.1511** **-0.1146** **-0.1625** |\n",
      "| | Both |  **0.2122** **0.1091** **0.2383** | 0.0735 **-0.1737** 0.0342 |\n",
      "\n",
      "Test/Test AUPR\n",
      "| | Coord. |  **0.0890** **0.0954** **0.1865** | -0.0452 **-0.1412** 0.0008 |\n",
      "| | Angles |  **0.1623** -0.0290 **0.1500** | 0.0878 **-0.1529** **-0.2629** |\n",
      "| | Both |  **0.1587** **0.0857** **0.1591** | -0.0005 **-0.2397** -0.0207 |\n",
      "\n",
      "Test/Test accuracy\n",
      "| | Coord. |  0.0671 0.0540 **0.1980** | **0.0939** -0.0122 -0.0277 |\n",
      "| | Angles |  **0.1912** -0.0235 **0.2000** | **0.0504** **-0.0693** **-0.0876** |\n",
      "| | Both |  **0.1271** **0.0987** **0.1863** | 0.0426 0.0143 0.0098 |\n",
      "\n",
      "\n",
      "p-values\n",
      "Test/Test AUROC\n",
      "| | Coord. |  **0.0213** **0.0016** **0.0000** | 0.3859 0.0908 0.8785 |\n",
      "| | Angles |  **0.0000** 0.8431 **0.0000** | **0.0027** **0.0192** **0.0030** |\n",
      "| | Both |  **0.0000** **0.0069** **0.0000** | 0.1381 **0.0004** 0.5183 |\n",
      "\n",
      "Test/Test AUPR\n",
      "| | Coord. |  **0.0070** **0.0036** **0.0000** | 0.2423 **0.0063** 0.9875 |\n",
      "| | Angles |  **0.0000** 0.3985 **0.0000** | 0.0909 **0.0037** **0.0000** |\n",
      "| | Both |  **0.0000** **0.0108** **0.0000** | 0.9901 **0.0000** 0.7011 |\n",
      "\n",
      "Test/Test accuracy\n",
      "| | Coord. |  0.0547 0.0840 **0.0000** | **0.0031** 0.6917 0.4870 |\n",
      "| | Angles |  **0.0000** 0.4485 **0.0000** | **0.0427** **0.0248** **0.0043** |\n",
      "| | Both |  **0.0005** **0.0072** **0.0000** | 0.1324 0.6787 0.7724 |\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print difference table rows (markdown)\n",
    "feature_type_map = {\n",
    "    \"coordinates\": \"Coord.\",\n",
    "    \"angles\": \"Angles\",\n",
    "    \"both\": \"Both\"\n",
    "}\n",
    "\n",
    "print(\"Differences\")\n",
    "for metric in METRIC:\n",
    "    print(metric)\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        line = f\"| | {feature_type_map[feature_type]} | \"\n",
    "        for type_a in TYPE_A:\n",
    "            for model in MODELS:\n",
    "                val = table_value_dict[type_a][feature_type][model][metric]['diff']\n",
    "                val_str = f\"**{val:.4f}**\" if table_value_dict[type_a][feature_type][model][metric]['p-value'] < 0.05 else f\"{val:.4f}\"\n",
    "                line += f\" {val_str}\"\n",
    "                \n",
    "            line += \" |\"\n",
    "\n",
    "        print(line)\n",
    "    print(\"\")\n",
    "\n",
    "print(\"\")\n",
    "# Print p-value table rows (markdown)\n",
    "print(\"p-values\")\n",
    "for metric in METRIC:\n",
    "    print(metric)\n",
    "    for feature_type in FEATURE_TYPES:\n",
    "        line = f\"| | {feature_type_map[feature_type]} | \"\n",
    "        for type_a in TYPE_A:\n",
    "            for model in MODELS:\n",
    "                val = table_value_dict[type_a][feature_type][model][metric]['p-value']\n",
    "                val_str = f\"**{val:.4f}**\" if table_value_dict[type_a][feature_type][model][metric]['p-value'] < 0.05 else f\"{val:.4f}\"\n",
    "                line += f\" {val_str}\"\n",
    "            \n",
    "            line += \" |\"\n",
    "\n",
    "        print(line)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GMA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
